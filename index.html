<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

    <meta name="keywords"
          content="iccv, workshop, computer vision, computer graphics, visual learning, simulation environments, robotics, machine learning, natural language processing, reinforcement learning">

    <link rel="shortcut icon" href="2022/img/website_logo2022.png">


    <title>Urban3D: The 2nd Challenge on Large Scale Point-cloud Analysis for Urban Scenes Understanding</title>
    <meta name="description"
          content="Urban3D: The 2nd Challenge on Large Scale Point-cloud Analysis for Urban Scenes Understanding ---">

    <!--Open Graph Related Stuff-->
    <meta property="og:title" content="Challenge on Large Scale Point-cloud Analysis for Urban Scenes Understanding"/>
    <meta property="og:url" content="https://urban3dchallenge.github.io/"/>
    <meta property="og:description"
          content="Urban3D: The 2nd Challenge on Large Scale Point-cloud Analysis for Urban Scenes Understanding ---"/>
    <meta property="og:site_name"
          content="Urban3D: The 2nd Challenge on Large Scale Point-cloud Analysis for Urban Scenes Understanding"/>
    <meta property="og:image" content=""/>
    <meta property="og:image:url" content=""/>

    <!--Twitter Card Stuff-->
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:title"
          content="Urban3D: The 1st Challenge on Large Scale Point-cloud Analysis for Urban Scenes Understanding"/>
    <meta name="twitter:image" content="https://github.com/Urban3DChallenge/2022/img/website_logo2022.png">
    <meta name="twitter:url" content="urban3dchallenge.github.io"/>
    <meta name="twitter:description"
          content="Urban3D: The 2nd Challenge on Large Scale Point-cloud Analysis for Urban Scenes Understanding ---"/>

    <!-- CSS  -->
    <link rel="stylesheet" type="text/css" href="./2022/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="./2022/css/main.css?2" media="screen,projection">

    <!-- Font Awesome -->
    <script src="/static/js/jquery.min.js?1"></script>
    <script src="https://kit.fontawesome.com/ff6e9b10da.js" crossorigin="anonymous"></script>
    <script src="/static/js/moment.min.js?1"></script>
    <script src="/static/js/main.js?2"></script>
</head>

<body>

<style>
    #year-header {
        border: none;
        display: block;
        width: 100%;
        background: #eaebea;
    }

    #year-header ul {
        display: block;
        margin: 0;
        margin-block-start: 0;
        margin-block-end: 0;
        padding-inline-start: 0;
        text-align: center;
    }

    #year-header ul li {
        display: inline-block;
        list-style: none;
    }

    #year-header ul li a {
        display: block;
        text-decoration: none;
        border: none;

        padding: 0 1.7em;
        height: 3.2em;
        line-height: 3.2em;
        vertical-align: middle;

        font-family: Helvetica, Arial, sans-serif;
        font-size: 1.0em;
        font-weight: 400;
        color: #444;
    }

    @media (max-width: 1000px) {
        #year-header ul li a {
            font-size: 0.8em;
        }
    }

    #year-header ul li a:hover {
        background: #dddedd;
        color: #000;
        transition: .5s;
    }
</style>

<div class="navbar navbar-default sticky-top">
    <div class="container">

        <div class="navbar-header">
            <a class="navbar-brand" href="/"></a>
            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>


        <div class="navbar-collapse collapse" id="navbar-main">
            <ul class="nav navbar-nav">
                <li><a href="#intro">Introduction</a></li>
                <li><a href="#calls">Call for Contributions</a></li>
                <li><a href="#dates">Important Dates</a></li>
                <!--<li><a href="#schedule">Schedule</a></li>-->
                <li><a href="#speakers">Invited Speakers</a></li>
                <!-- <li><a href="#awards">Awards</a></li> -->
                <li><a href="#Invited Panel Speakers">Panel</a></li>
                <!--                <li><a href="#accepted">Accepted Papers</a></li>-->
                <li><a href="#organizers">Organizers</a></li>
                <!--<li><a href="#programcommittee">Program Committee</a></li>-->
                <li><a href="#sponsors">Sponsors</a></li>

                <li><a href="#sponsors">Previous</a></li>

            </ul>
        </div>

    </div>
</div>


<div class="container">
    <div class="page-content">
        <p><br/></p>
        <div class="row">
            <div class="col-xs-12">
                <img class="img-fluid" src="2022/img/website_logo2022.png"/>
                <!--                <small style="float:right;margin-top:1mm;margin-right:5mm;">Image credit to <a-->
                <!--                        href="https://pixabay.com/users/danielhannah-8058574" target="_blank">Daniel Hannah</a></small>-->
                <!--<center>Date TBD, half-day</center>-->
            </div>
        </div>
        <p><br/></p>

        <div class="row">
            <div class="col-xs-12"><a class="anchor" id="intro"></a>
                <h2>Introduction</h2>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12">
                <p>
                    The 3D world around us is composed of a rich variety of objects: <i> buildings, bridges, trees,
                    cars,
                    rivers,</i> and so forth, each with distinct appearance, morphology, and function. Giving machines
                    the
                    ability to precisely segment and label these diverse objects is of key importance to allow them to
                    interact competently within our physical world, for applications such as scene-level robot
                    navigation, autonomous driving, and even large-scale urban 3D modeling, which is critical for the
                    future of smart city planning and management.
                </p>
                <p>
                    Over the past years, remarkable advances in techniques for 3D point cloud understanding have greatly
                    boosted performance. Although these approaches achieve impressive results for object recognition
                    and semantic segmentation, almost all of them are limited to extremely small 3D point clouds, and
                    are difficult to be directly extended to large-scale point clouds. 
			<font color="red"><b><a
                        href="http://live.bilibili.com/23697730" target="_blank">[Bilibili Live]</a></b></font>
			<font color="red"><b><a
                        href="https://www.youtube.com/watch?v=egBFjbQH8CE" target="_blank">[YouTube Live]</a></b></font>


                </p>
                <p>
                    <!-- <b>The 2nd Challenge on Large Scale Point-cloud Analysis for Urban Scenes Understanding
                        (Urban3D) </b> at <a
                        href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a> aims to establish a new
                    benchmark for 3D semantic segmentation on urban-scale point clouds. In particular, we prime the
                    challenge with a dataset, called <b><a
                        href="http://point-cloud-analysis.cs.ox.ac.uk/" target="_blank">SensatUrban</a></b>, which
                    consists of large-scale subsections of multiple
                    urban areas in the UK. With the high quality of per-point annotations and the diverse distribution
                    of semantic categories, the SensatUrban dataset allows us to explore a number of key research
                    problems and directions for 3D semantic learning in this workshop. We aspire to highlight the
                    challenges faced in 3D semantic segmentation on extremely large and dense point clouds of urban
                    environments, sparking innovation in applications such as smart cities, digital twins, autonomous
                    vehicles, automated asset management of large national infrastructures, and intelligent
                    construction sites. We hope that our dataset, and this workshop could inspire the community to
                    explore the next level of 3D semantic learning. Specifically, We encourage researchers from a wide
                    range of background to participate in our challenge, the topics including but not limited to:
                </p> -->
                    <b>The 2nd Challenge on Large Scale Point-cloud Analysis for Urban Scenes Understanding
                        (Urban3D) </b> at <a
                        href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a> aims to establish new
                    benchmarks for 3D semantic and instance segmentation on urban-scale point clouds. In particular, we prime the
                    challenge with both <b><a
                        href="http://point-cloud-analysis.cs.ox.ac.uk/" target="_blank">SensatUrban</a></b> and <b><a
                        href="https://www.stpls3d.com//" target="_blank">STPLS3D</a></b> datasets. SensatUrban consists of large-scale subsections of multiple
                    urban areas in the UK. With the high quality of per-point annotations and the diverse distribution
                    of semantic categories. STPLS3D is composed of both real-world and synthetic environments which cover more than 17 km<sup>2</sup> of the city landscape in the U.S. with up to 18 fine-grained semantic classes and 14 instance classes.  
                    These two datasets are complementary to each other and allow us to explore a number of key research
                    problems and directions for 3D semantic and instance learning in this workshop. We aspire to highlight the
                    challenges faced in 3D segmentation on extremely large and dense point clouds of urban
                    environments, sparking innovation in applications such as smart cities, digital twins, autonomous
                    vehicles, automated asset management of large national infrastructures, and intelligent
                    construction sites. We hope that our datasets, and this workshop could inspire the community to
                    explore the next level of 3D learning. Specifically, We encourage researchers from a wide
                    range of background to participate in our challenge, the topics including but not limited to:
                </p>
                <ul>
                    <li>Semantic segmentation of large-scale 3D point clouds.
                    </li>
                    <li>Instance segmentation of 3D point clouds.
                    </li>
                    <li>Weakly supervised learning in 3D point clouds analysis.</li>
                    <li>Learning from imbalanced 3D point clouds.
                    </li>
                    <li>3D point cloud acquisition & visualization.</li>
                    <li>3D object detection & reconstruction.</li>
                    <li>Semi-/weak-/un-/self- supervised learning methods for 3D point clouds.
                    </li>
                </ul>
                We will be hosting 2 invited speakers and holding 2 parallel challenges (i.e., semantic and instance segmentation), and 1 panel discussion session for the topic of point cloud segmentation. More
                information will be provided as soon as possible.
            </div>
        </div>
        <p><br/></p>

        <div class="row">
            <div class="col-xs-12 panel-group"><a class="anchor" id="calls"></a>
                <h2>Call for Contributions</h2>
                <!--                <br/>-->
                <!--                <div class="panel panel-default">-->
                <!--                    <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-papers"-->
                <!--                         style="cursor:pointer;">-->
                <!--                        <h3 style="margin:0;">Full Workshop Papers</h3>-->
                <!--                    </div>-->
                <!--                    <div id="call-papers" class="panel-collapse collapse" data-parent="#call">-->
                <!--                        <div class="panel-body">-->
                <!--                            <p>-->
                <!--                                <span style="font-weight:500;">Submission:</span> We invite authors to submit-->
                <!--                                unpublished papers (8-page <a-->
                <!--                                    href="http://iccv2021.thecvf.com/node/4#submission-guidelines" target="_blank">ICCV-->
                <!--                                format</a>) to our workshop, to be presented at a poster session upon acceptance. All-->
                <!--                                submissions will go through a double-blind review process. All contributions must be-->
                <!--                                submitted (along with supplementary materials, if any) at-->
                <!--                                <a href="https://cmt3.research.microsoft.com/GAZE2021/Submission/Index" target="_blank"-->
                <!--                                   title="CMT Submission System for GAZE 2021">this CMT link</a>.-->
                <!--                            </p>-->
                <!--                            <p>-->
                <!--                                Accepted papers will be published in the official ICCV Workshops proceedings and the-->
                <!--                                Computer Vision Foundation (CVF) Open Access archive.-->
                <!--                            </p>-->
                <!--                            <p>-->
                <!--                                <span style="font-weight:500;">Note:</span> Authors of previously rejected main-->
                <!--                                conference submissions are also welcome to submit their work to our workshop. When doing-->
                <!--                                so, you must submit the previous reviewers' comments (named as <code>previous_reviews.pdf</code>)-->
                <!--                                and a letter of changes (named as <code>letter_of_changes.pdf</code>) as part of your-->
                <!--                                supplementary materials to clearly demonstrate the changes made to address the comments-->
                <!--                                made by previous reviewers.-->
                <!--                                &lt;!&ndash;Due to potential clashes with the main conference reviewing schedule, we will accept simultaneous submissions to the ICCV main conference and GAZE Workshop. Simultaneous submissions are otherwise disallowed.&ndash;&gt;-->
                <!--                            </p>-->
                <!--                        </div>-->
                <!--                    </div>-->
                <!--                </div>-->
                <!--                <br/>-->
                <!--                &lt;!&ndash;-->
                <!--                <div class="panel panel-default">-->
                <!--                  <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-ea" style="cursor:pointer;">-->
                <!--                    <h3 style="margin:0;">Extended Abstracts</h3>-->
                <!--                  </div>-->
                <!--                  <div id="call-ea" class="panel-collapse collapse in" data-parent="#call">-->
                <!--                    <div class="panel-body">-->
                <!--                      <p>-->
                <!--                        In addition to regular papers, we also invite extended abstracts of ongoing or published work (<i>e.g.</i> related papers on ECCV main track). The extended abstracts will not be published or made available to the public (we will only list titles on our website) but will rather be presented during our poster session. We see this as an opportunity for authors to promote their work to an interested audience to gather valuable feedback.-->
                <!--                      </p>-->
                <!--                  <p>-->
                <!--                    Further details on how this poster session will occur online, is to be decided. In general, we will be following the main ECCV conference guidelines and organization in organizing the presentation of all posters.-->
                <!--                      </p>-->
                <!--                      <p>Extended abstracts are limited to six pages and must be created using the <a href="https://eccv2020.eu/author-instructions/" target="_blank">official ECCV format</a>. The submission must be sent to <a href="mailto:openeyes.workshop@gmail.com">openeyes.workshop@gmail.com</a> by the deadline (July 17th).-->
                <!--                      </p>-->
                <!--                      <p>-->
                <!--                        We will evaluate and notify authors of acceptance as soon as possible (evaluation on a rolling basis until the deadline) after receiving their extended abstract submission.-->
                <!--                      </p>-->
                <!--                    </div>-->
                <!--                  </div>-->
                <!--                </div>-->
                <!--                <br>-->
                <!--                &ndash;&gt;-->
                <div class="panel panel-default">
                    <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-challenge"
                         style="cursor:pointer;">
                        <h3 style="margin:0;">Urban3D Challenges@ECCV'2022</h3>
                    </div>
                    <div id="call-challenge" class="panel-collapse collapse in" data-parent="#call">
                        <div class="panel-body">
                            <p>
                                <b>The Urban3D Challenges are hosted on Codalab, and can be found at:</b>
                            </p>
                            <ul>
                                <b>Track 1:</b> 3D Semantic Segmentation of Urban-scale Point Clouds.
                                <ul>
                                    <li><font color="red"><b>Urban3D Challenge</b></font>: <a href="https://codalab.lisn.upsaclay.fr/competitions/7113">https://codalab.lisn.upsaclay.fr/competitions/7113</a>
                                    </li>
                                    <li>SensatUrban dataset: <a href="http://point-cloud-analysis.cs.ox.ac.uk/">http://point-cloud-analysis.cs.ox.ac.uk/</a>
                                    </li>
                                    <li>SensatUrban API: <a href="https://github.com/QingyongHu/SensatUrban">https://github.com/QingyongHu/SensatUrban</a>
                                    </li>
                                </ul>
                            </ul>
                            <ul>
                                <b>Track 2:</b> 3D Instance Segmentation of Urban-scale Point Clouds.
                                <ul>
                                    <li><font color="red"><b>STPLS3D Challenge</b></font>: <a href="https://codalab.lisn.upsaclay.fr/competitions/4646">https://codalab.lisn.upsaclay.fr/competitions/4646</a>
                                    </li>
                                    <li>STPLS3D dataset: <a href="https://www.stpls3d.com">https://www.stpls3d.com</a>
                                    </li>
                                    <li>STPLS3D API: <a href="https://github.com/meidachen/STPLS3D">https://github.com/meidachen/STPLS3D</a>
                                    </li>
                                </ul>
                            </ul>
                            <!-- <p>
                                More information on the respective challenges can be found on their pages.
                            </p> -->
                            <br/>
                                <p>
                                    We are thankful to our sponsor for providing the following prizes. The prize award will be granted to the <b>Top 3</b> individuals and teams for <b>Each Challenge Track</b> on the leaderboard that provide a valid submission.
                                <table style="width: 100%;">
                                    <colgroup>
                                        <col span="1" style="width: 50%;"/>
                                        <col span="1" style="width: 35%;"/>
                                        <col span="1" style="width: 15%;"/>
                                    </colgroup>
                                    <tbody>
                                        <tr>
                                            
                                            <td><ul><ul><li><b>1st Place:</b></li></ul></ul></td>
                                            <td>$1,500 USD</td>
                                            <td><small>courtesy of </small><img width="50" src="2022/img/USC_ICT_Logo.png"/></td>

                                        </tr>
                                        <tr>
                                            <td><ul><ul><li><b>2nd Place:</b></li></ul></ul></td>
                                            <td>$1,000 USD</td>
                                            <td><small>courtesy of </small><img width="50" src="2022/img/USC_ICT_Logo.png"/></td>
                                        </tr>
                                        <tr>
                                            <td><ul><ul><li><b>3rd Place:</b></li></ul></ul></td>
                                            <td>$500 USD</td>
                                            <td><small>courtesy of </small><img width="50" src="2022/img/USC_ICT_Logo.png"/></td>
                                        </tr>
                                        <!-- <tr>
                                            <td><b>EVE Challenge Winner</b></td>
                                            <td>Tobii Eye Tracker 5</td>
                                            <td><small>courtesy of </small><img width="50" src="2021/img/tobii.jpg"/></td>
                                        </tr> -->
                                    </tbody>
                                </table>
                                </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <p><br/></p>

        <div class="row">
            <div class="col-xs-12"><a class="anchor" id="dates"></a>
                <h2>Important Dates</h2>
                <br/>
                <table class="table table-striped">
                    <tbody>
                    <tr>
                        <td>Workshop Proposal Accepted</td>
                        <td>March 26, 2022</td>
                    </tr>
                    <tr>
                        <td>Competition Starts</td>
                        <td>May 12, 2022</td>
                    </tr>
                    <tr>
                        <td>Competition Ends</td>
                        <td>October 10, 2022 (23:59 Pacific time)</td>
                    </tr>
                    <tr>
                        <td>Notification to Participants</td>
                        <td>October 15, 2022</td>
                    </tr>
                    <tr>
                        <td>Finalized Workshop Program (Half Day)</td>
                        <td>October 23, 2022 (9:00-13:00 IDT (UTC+3))</td>
                    </tr>
                    <tr id="schedule">
                        <td></td>
                        <td></td>
                    </tr>
                    </tbody>
                </table>
            </div>
        </div>
        <p><br/></p>

        <div class="row">
            <div class="col-xs-12"><a class="anchor" id="dates"></a>
                <h2>Preliminary Program Outline (TBD)</h2>
                <br/>
                <table class="table table-striped">
                    <tbody>
                    <tr>
                        <td>09:00-09:05</td>
                        <td>Welcome Introduction</td>
                    </tr>
                    <tr>
                        <td>09:05-09:50</td>
                        <td>Invited Talk (Talk 1)</td>
                    </tr>
                    <tr>
                        <td>09:50-10:35</td>
                        <td>Invited Talk (Talk 2)</td>
                    </tr>
                    <tr>
                        <td>10:35-10:40</td>
                        <td>Coffee break</td>
                    </tr>
                    <tr>
                        <td>10:40-10:50</td>
                        <td>Awarding ceremony</td>
                    </tr>
                    <tr>
                        <td>10:50-11:50</td>
                        <td>Winner Talk (Track 1) + Q&A</td>
                    </tr>
                    <tr>
                        <td>11:50-12:50</td>
                        <td>Winner Talk (Track 2) + Q&A</td>
                    </tr>
                    <tr>
                        <td>12:50-13:00</td>
                        <td>Closing Remarks</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td></td>
                    </tr>
                    </tbody>
                </table>
            </div>
        </div>
        <p><br/></p>


    <div class="row">
        <div class="col-xs-12"><a class="anchor" id="speakers"></a>
            <h2>Invited Keynote Speakers</h2>
            <br/>
            <div class="row speaker">
                <div class="col-sm-3 speaker-pic">
                    <a href="https://ict.usc.edu/about-us/leadership/senior-management/randall-hill-jr/">
                        <img class="people-pic" src="./2022/img/people/randall.jpg"/>
                    </a>
                    <div class="people-name">
                        <a href="https://ict.usc.edu/about-us/leadership/senior-management/randall-hill-jr/">Randall W. Hill, Jr. 
                        </a>
                        <h6>USC Institute for Creative Technologies</h6>
                    </div>
                </div>
                <div class="col-sm-9">
                    <h3>3D point cloud classification with an eye on daily applications</h3><br/>
                    <!--<b>Abstract</b><p class="speaker-abstract"></p>-->
                    <div class="panel panel-default">
                        <div class="panel-heading" data-toggle="collapse" href="#jr-bio"
                             style="cursor:pointer;text-align:center">
                            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
                        </div>
                        <div id="jr-bio" class="panel-collapse collapse in">
                            <div class="panel-body">
                                <p class="speaker-bio">
                                    Randall W. Hill, Jr. became the executive director of the USC Institute for Creative Technologies in 2006.  A leader in understanding how classic storytelling and high-tech tools can create meaningful learning experiences, Hill steers the institute’s exploration of how virtual humans, mixed reality worlds, advanced computer graphics, dramatic films, social simulations and educational videogames can augment more traditional methods for imparting lessons. He oversees a diverse team of scientists, storytellers, artists and educators as they pioneer and evaluate new ways to deliver effective teaching and training in areas including leadership, cultural awareness, negotiation and mental health treatment and assessment.
                                    <br />
                                    <br />
                                    He is a research professor in the computer science department at the USC Viterbi School of Engineering. His research focus is on using intelligent tutoring systems and virtual humans to augment immersive learning environments.  Hill’s career at USC began in 1995 at the USC Information Sciences Institute where he worked on the development of models of human behavior and decision-making for real-time simulation environments. He joined the USC Institute for Creative Technologies in 2000 as a senior scientist.  Prior to joining USC, Hill served as a group supervisor and the work area manager for network automation in the Deep Space Network Advanced Technology Program at NASA’s Jet Propulsion Laboratory.
                                    <br />
                                    <br />
                                    Hill graduated with a Bachelor of Science degree from the United States Military Academy at West Point and earned his M.S. and Ph.D. degrees in computer science from the University of Southern California. 
                                </p>
                            </div>
                        </div>
                    </div>
                    <div class="panel-heading" data-toggle="collapse" href="#jr-bio"
                         style="cursor:pointer;text-align:center">

                    </div>
                </div>
            </div>


        <div class="row">
            <div class="col-xs-12"><a class="anchor" id="speakers"></a>
                <h2>Invited Keynote Speakers</h2>
                <br/>
                <div class="row speaker">
                    <div class="col-sm-3 speaker-pic">
                        <a href="http://www.lsgi.polyu.edu.hk/people/academic/shi-wen-zhong/index.asp">
                            <img class="people-pic" src="./2022/img/people/shi-wen-zhong.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="http://www.lsgi.polyu.edu.hk/people/academic/shi-wen-zhong/index.asp">SHI Wen-zhong
                            </a>
                            <h6>The Hong Kong Polytechnic University</h6>
                        </div>
                    </div>
                    <div class="col-sm-9">
                        <h3>3D point cloud classification with an eye on daily applications</h3><br/>
                        <!--<b>Abstract</b><p class="speaker-abstract"></p>-->
                        <div class="panel panel-default">
                            <div class="panel-heading" data-toggle="collapse" href="#jr-bio"
                                 style="cursor:pointer;text-align:center">
                                <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
                            </div>
                            <div id="jr-bio" class="panel-collapse collapse in">
                                <div class="panel-body">
                                    <p class="speaker-bio">
                                        Professor John Shi is currently the Director of Otto Poon Charitable Foundation Smart Cities Research Institute of PolyU, Director of PolyU-Shenzhen Technology and Innovation Research Institute (Futian), Chair Professor in Geographic Science and Remote Sensing, and Director of Joint Research Laboratory on Spatial Information of PolyU and Wuhan University. He is Academician of International Eurasian Academy of Sciences and Fellow of Academy of Social Sciences (UK). He earned his doctoral degree from University of Osnabruck in Vechta, Germany in 1994. A Fellow of Royal Institution of Chartered Surveyors and Hong Kong Institute of Surveyors, Professor Shi also serves as President of International Society for Urban Informatics and Editor-in-Chief of International Journal Urban Informatics. 
                                        <br />
                                        <br />
                                        His research covers urban informatics for smart cities, geo-informatic science and remote sensing, artificial-intelligence-based object extraction and change detection from satellite imagery, intelligent analytics and quality control for spatial big data, and mobile mapping and 3-D modelling based on LiDAR and remote sensing imagery. He has published almost 300 research articles in journals indexed by Web of Science and 20 books. He is among the worldly top 2% cited researchers according to the standardized citation indicators published by Elsevier BV and scholar in Stanford University. He has 34 patents grants and 33 other patent applications filed.
                                        <br />
                                        <br />
                                        Professor Shi has won Natural Science Award, China’s highest award for fundamental research, in 2007; Distinguished Scholar Prize by CPGIS, Gold Medal in Geneva Invention Expo, and Smart 50 Awards (US) in 2021; Founder’s Award by International Spatial Accuracy Research Association in 2020; China’s Science and Technology Progress Award in Surveying and Mapping (Grand Award) in 2017; Wang Zhizhuo Award by International Society of Photogrammetry and Remote Sensing in 2012; and ESRI Award for Best Scientific Paper in Geo-informatic Science by American Society of Photogrammetry and Remote Sensing in 2006.
                                    </p>
                                </div>
                            </div>
                        </div>
                        <div class="panel-heading" data-toggle="collapse" href="#jr-bio"
                             style="cursor:pointer;text-align:center">

                        </div>
                    </div>
                </div>


                <div class="row speaker">
                    <div class="col-sm-3 speaker-pic">
                        <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">
                            <img class="people-pic" src="./2022/img/people/Gerard Pons-Moll.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll
                            </a>
                            <h6><a href="https://virtualhumans.mpi-inf.mpg.de/">University of Tübingen</a></h6>
                        </div>
                    </div>
                    <div class="col-sm-9">
                        <h3>Virtual Humans — From appearance to behaviour</h3><br/>
                        <!--<b>Abstract</b><p class="speaker-abstract"></p>-->
                        <div class="panel panel-default">
                            <div class="panel-heading" data-toggle="collapse" href="#jr-bio"
                                 style="cursor:pointer;text-align:center">
                                <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
                            </div>
                            <div id="jr-bio" class="panel-collapse collapse in">
                                <div class="panel-body">
                                    <p class="speaker-bio">
                                        Gerard Pons-Moll is a Professor at the University of Tübingen, at the department of Computer Science. He is also the head of the Emmy Noether independent research group "Real Virtual Humans", senior researcher at the Max Planck for Informatics (MPII) in Saarbrücken, Germany, and faculty at the IMPRS-IS (International Max Planck Research School - Intelligent Systems in Tübingen) and faculty at Saarland Informatics Campus. His research lies at the intersection of computer vision, computer graphics and machine learning -- with special focus on analyzing people in videos, and creating virtual human models by "looking" at real ones. His research has produced some of the most advanced statistical human body models of pose, shape, soft-tissue and clothing (which are currently used for a number of applications in industry and research), as well as algorithms to track and reconstruct 3D people models from images, video, depth, and IMUs.

                                        <br />
                                        <br />

                                        His work received several awards such as the Emmy Noether grant’18, German Pattern Recognition Award’19, and best papers and nominations at CVPR’21, CVPR’20, IVA’21, 3DV’22, 3DV’18, Eurographics’17 and BMVC’2013. 
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
		    
		 <!--    <div class="row">
                    <div class="col-xs-12"><a class="anchor" id="Invited Panel Speakers"></a>
                        <h2>Invited Panel Speakers</h2>
                    </div>
                </div>

                <div class="row">
                    <div class="col-xs-1"></div>
                    <div class="col-xs-2">
                        <a href="https://hszhao.github.io/">
                            <img class="people-pic" src="2022/img/people/hengshuangzhao.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://hszhao.github.io/">Hengshuang Zhao</a>
                            <h6>The University of Hong Kong (HKU)</h6>
                        </div>
                    </div>
                    <div class="col-xs-2">
                        <a href="http://tbirdal.me/">
                            <img class="people-pic" src="2022/img/people/Tolga.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="http://tbirdal.me/">Tolga Birdal</a>
                            <h6>Stanford University</h6>
                        </div>
                    </div>
                    <div class="col-xs-2">
                        <a href="https://people.csail.mit.edu/yuewang/">
                            <img class="people-pic" src="2022/img/people/Yue_wang.png"/>
                        </a>
                        <div class="people-name">
                            <a href="https://people.csail.mit.edu/yuewang/">Yue Wang</a>
                            <h6>Massachusetts Institute of Technology</h6>
                        </div>
                    </div>
			
		   <div class="col-xs-2">
                        <a href="https://pengsongyou.github.io/">
                            <img class="people-pic" src="2022/img/people/songyou.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://pengsongyou.github.io/">Songyou Peng</a>
                            <h6> ETH Zurich and Max Planck Institute</h6>
                        </div>
                    </div>
			
                    <div class="col-xs-2">
                        <a href="https://udayton.edu/directory/engineering/electrical_and_computer/singer-nina.php">
                            <img class="people-pic" src="2022/img/people/varney-nina.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://udayton.edu/directory/engineering/electrical_and_computer/singer-nina.php/">Nina Singer</a>
                            <h6>University of Dayton</h6>
                        </div>
                    </div>
		          <p><br/></p> -->
			
<!-- 		    <div class="col-xs-2">
                        <a href="https://yanx27.github.io/">
                            <img class="people-pic" src="2021/img/people/xu.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://yanx27.github.io/">Xu Yan</a>
                            <h6>The Chinese University of Hong Kong, Shenzhen</h6>
                        </div>
                    </div>
		
		    <div class="col-xs-2">
                        <a href="https://www.researchgate.net/profile/Zhuoxu-Huang">
                            <img class="people-pic" src="2021/img/people/zhuoxu.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://www.researchgate.net/profile/Zhuoxu-Huang">Zhuoxu Huang</a>
                            <h6>Wuhan University</h6>
                        </div>
                    </div> -->
<!-- 			
			
                    
                </div>
                <p><br/></p> -->


<!--                 <div class="row">
                    <div class="col-xs-12"><a class="anchor" id="awards"></a>
                        <h2>Awards</h2>
                        <div class="award">
                            <h3> Winner Award
                                <span class="award-sponsor">sponsored by <a href="https://www.sensat.co/"
                                                                            target="_blank"><img
                                        src="./2021/img/sensat.png"/></a>
	    </span></h3>
                            <p><br/>
                                Team <b>BUPT.BIGO.GAGO.WHU</b><br/>
				<b>IDS-Net: Improved Down-sampling Method for Semantic Segmentation of Class-imbalanced Large-Scale Point Clouds</b><br />
                                <i>Zeqiang Wei, Kai Jin, Kuan Song, Tianyu Xiu, Xiuzhuang Zhou, and Guodong Guo</i>
                            </p>
                        </div>

                        <div class="award">
                            <h3> 2nd Place Award
                                <span class="award-sponsor">	    </span></h3>
                            <p><br/>
                                Team <b>Deep Bit Lab</b><br/>
				<b>City-scale point cloud semantic segmentation via coarse-to-fine small object refinement</b><br />
                                <i>Xu Yan, Zhen Li, Chaoda Zheng, Haiming Zhang, Jiantao Gao, Wending Zhou, Yinghong
                                    Liao, Zhihao Yuan, Sheng Wang, Shuguang Cui</i>
                            </p>
                        </div>

                         <div class="award">
                            <h3> 3rd Place Award
                                <span class="award-sponsor">	    </span></h3>
                            <p><br/>
                                Team <b>what‘s up?</b><br/>
				<b>Boosting Semantic Segmentation in Urban-Scale Point Clouds via Transformers</b><br />
                                <i>Zhuoxu Huang, Zhiyou Zhao, Banghuai Li, Huabin Huang, Ye Yuan</i>
                            </p>
                        </div>



                            </p>
                        </div>
                    </div>
                </div>
                <p><br/></p> -->


                <div class="row">
                    <div class="col-xs-12"><a class="anchor" id="organizers"></a>
                        <h2>Organizers</h2>
                    </div>
                </div>

                <div class="row">
                    <div class="col-xs-1"></div>
                    <div class="col-xs-2">
                        <a href="https://qingyonghu.github.io/">
                            <img class="people-pic" src="2022/img/people/qingyong.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://qingyonghu.github.io/">Qingyong Hu</a>
                            <h6>University of Oxford</h6>
                        </div>
                    </div>
                    <div class="col-xs-2">
                        <a href="https://scholar.google.com/citations?user=ii7ZwfQAAAAJ">
                            <img class="people-pic" src="2022/img/people/Meida.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://www.linkedin.com/in/meida-chen-938a265b/">Meida Chen</a>
                            <h6>University of Southern California - Institute for Creative Technologies</h6>
                        </div>
                    </div>
                    <div class="col-xs-2">
                        <a href="https://www.cs.ox.ac.uk/people/ta-ying.cheng/">
                            <img class="people-pic" src="2022/img/people/Ta-Ying.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://www.cs.ox.ac.uk/people/ta-ying.cheng/">Ta-Ying Cheng</a>
                            <h6>University of Oxford</h6>
                        </div>
                    </div>
                    <div class="col-xs-2">
                        <a href="https://uk.linkedin.com/in/fakharkhalid">
                            <img class="people-pic" src="2022/img/people/Khalid_7.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://uk.linkedin.com/in/fakharkhalid">Sheikh Khalid</a>
                            <h6>Sensat LTD.</h6>
                        </div>
                    </div>
                    <div class="col-xs-2">
                        <a href="https://yang7879.github.io/">
                            <img class="people-pic" src="2022/img/people/BoYang.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://yang7879.github.io/">Bo Yang</a>
                            <h6>The Hong Kong Polytechnic University</h6>
                        </div>
                    </div>
                <div class="col-xs-1"></div>
                </div>

                <p><br/></p>
                <div class="row">
                    <div class="col-xs-1"></div>
                    <div class="col-xs-2">
                        <a href="http://www.ronnieclark.co.uk/">
                            <img class="people-pic" src="2022/img/people/Ronnie.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="http://www.ronnieclark.co.uk/">Ronarld Clark</a>
                            <h6>Imperial College London</h6>
                        </div>
                    </div>

                   <div class="col-xs-2">
                        <a href="http://www.ronnieclark.co.uk/">
                            <img class="people-pic" src="2022/img/people/jiahui.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="http://www.ronnieclark.co.uk/">Jiahui Chen</a>
                            <h6>Sun Yat-sen University</h6>
                        </div>
                    </div>

                   <div class="col-xs-2">
                        <a href="http://www.ronnieclark.co.uk/">
                            <img class="people-pic" src="2022/img/people/leying.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="http://www.ronnieclark.co.uk/">Leying Zhang</a>
                            <h6>Sun Yat-sen University</h6>
                        </div>
                    </div>

                   <div class="col-xs-2">
                        <a href="http://www.ronnieclark.co.uk/">
                            <img class="people-pic" src="2022/img/people/rongkun.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="http://www.ronnieclark.co.uk/">Rongkun Yang</a>
                            <h6>Sun Yat-sen University</h6>
                        </div>
                    </div>

                    <div class="col-xs-2">
                        <a href="http://yulanguo.me/">
                            <img class="people-pic" src="2022/img/people/yulan.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="http://yulanguo.me/">Yulan Guo</a>
                            <h6>National University of Defense Technology</h6>
                        </div>
                    </div>
                <div class="col-xs-1"></div>
                </div>

                    <p><br/></p>
                    <div class="row">
                    <div class="col-xs-1"></div>
                    <div class="col-xs-2">
                        <a href="https://www.cs.bham.ac.uk/~leonarda/">
                            <img class="people-pic" src="2022/img/people/al.jpg"/>
                        </a>
                        <div class="people-name">
                            <a href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a>
                            <h6>University of Birmingham</h6>
                        </div>
                    </div>
                    <div class="col-xs-2">
                        <a href="https://www.cs.ox.ac.uk/people/niki.trigoni/">
                            <img class="people-pic" src="2022/img/people/Niki.png"/>
                        </a>
                        <div class="people-name">
                            <a href="https://www.cs.ox.ac.uk/people/niki.trigoni/">Niki Trigoni</a>
                            <h6>University of Oxford</h6>
                        </div>
                    </div>
                    <div class="col-xs-2">
                        <a href="https://www.cs.ox.ac.uk/people/andrew.markham/">
                            <img class="people-pic" src="2022/img/people/Andrew_markham.png"/>
                        </a>
                        <div class="people-name">
                            <a href="https://www.cs.ox.ac.uk/people/andrew.markham/">Andrew Markham</a>
                            <h6>University of Oxford</h6>
                        </div>
                    </div>
                </div>
                <p><br/></p>


                <div class="row">
                    <div class="col-xs-12"><a class="anchor" id="sponsors"></a>
                        <h2>Workshop sponsored by:</h2>
                    </div>
                </div>

                <div class="row">
                    <div class="col-xs-4 sponsor">
                        <a href="https://ict.usc.edu/"><img src="2022/img/USC_ICT_Logo.png"/></a>
                    </div>
                    <div class="col-xs-4 sponsor">
                        <a href="https://www.sensat.co.uk/"><img src="2022/img/sensat.png"/></a>
                    </div>
                    <!--                    <div class="col-xs-4 sponsor">-->
                    <!--                        <a href="https://www.tobii.com/"><img src="2021/img/tobii.jpg"/></a>-->
                    <!--                    </div>-->
                    <!--                    <div class="col-xs-4 sponsor">-->
                    <!--                        <a href="https://www.google.com/"><img src="2021/img/google.png"/></a>-->
                    <!--                    </div>-->
                </div>
                <p><br/></p>

                <div class="row">
                    <div class="col-xs-12"><a class="anchor" id="previous"></a>
                        <h2>Previous years' workshops:</h2>
                    </div>
                </div>

                <div class="row">
                    <div class="col-xs-12"><a class="anchor" id="previous"></a>
                        <ul><li>
                        <b>Urban3D@ICCV2021: </b> <font color="blue"><a href="https://urban3dchallenge.github.io/2021" target="_blank">https://urban3dchallenge.github.io/2021</a></font>
                        </li></ul>
                    </div>
                </div>


            </div>
        </div>

        <hr>
        <div class="section text-gray" id="footer">
            <div class="container">
                <div class="row">
                    <div class="col-sm-14" style="text-align:right;">
                        <small>This work is sponsored by the U.S. Army Research Laboratory (ARL) under contract number W911NF-14-D-0005. Statements, expressed opinions, and content included do not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.</small>
                    </div>
                    <br><br>
                </div>
            </div>
        </div>


        <script type="text/javascript" src="/static/js/jquery.min.js"></script>
        <script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
</body>
</html>
